{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import required packages"
   ],
   "metadata": {
    "id": "vnRQvI0bieNL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aavnuByVymwK",
    "outputId": "d277b9ef-fa1a-4a17-8872-3e5dfda91f6c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print(f\"Length of text: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Duhg9NrUymwO",
    "outputId": "d7049381-354b-4309-ae0b-3ef48524701e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IlCgQBRVymwR",
    "outputId": "1996ad57-5709-406a-b135-8ccef9f25541",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f\"{len(vocab)} unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "a86OoYtO01go",
    "outputId": "01ead5c2-b8b1-463e-fbbd-91a0db085094",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "example_texts = [\"abcdefg\", \"xyz\"]\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6GMlCe3qzaL9"
   },
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WLv5Q_2TC2pc",
    "outputId": "01ee4f98-6e7f-4021-82a3-abedc3728529",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Wd2m3mqkDjRj"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c2GCh0ySD44s",
    "outputId": "6a1d5366-bd71-47d0-e588-ee09fdeec715",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zxYI-PeltqKP",
    "outputId": "d5c0929c-66e6-44ae-cbf1-74ee806353d2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w5apvBDn9Ind"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cjH5v45-yqqH",
    "outputId": "0ac9d405-91a6-42a1-fdbc-8d1804625ddd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BpdjRO2CzOfZ",
    "outputId": "d4638697-8a6b-4fe9-f434-6267fafa7d78",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QO32cMWu4a06",
    "outputId": "4918dc15-cad4-44d2-d1e6-808ceb0e0cce",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9NGu-FkO_kYU",
    "outputId": "375a11cc-da1b-4027-d97a-4d1f7eb85ef7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "B9iKPXkw5xwa"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GNbw-iR0ymwj",
    "outputId": "ad4cde25-3b0e-4cc7-91dc-ab65c1583a38",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "## Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "p2pGotuNzf-S",
    "outputId": "54c20403-b5c8-400f-81cd-79bd659d3b21",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset.shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wj8HQ2w8z4iO"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            rnn_units, return_sequences=True, return_state=True\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IX58Xj9z47Aw"
   },
   "outputs": [],
   "source": [
    "model = MyModel(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "C-_70kKAPrPU",
    "outputId": "94b6da91-4303-464a-85eb-b1ee44481164",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(\n",
    "        example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vPGmAAXmVLGC",
    "outputId": "081b8e00-022e-4d7e-8969-0d303f767379",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YqFMUQc_UFgM",
    "outputId": "d4e834f3-d862-4072-ea7b-f09c76f1f220",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([33, 56, 25, 41, 35, 13, 44, 10, 14, 18,  9, 24, 53, 53, 11, 37, 44,\n",
       "       51,  6, 32, 41, 56, 15, 31, 29,  4, 16, 46, 58, 34, 14, 36, 27, 46,\n",
       "        0, 17, 26, 33, 13, 22,  9, 23, 10, 35, 16, 30, 32, 43, 12, 32, 29,\n",
       "       14, 35, 17, 39,  9, 51, 63, 58, 52, 55, 52, 20, 10,  7, 11, 63, 57,\n",
       "        4,  7,  7, 65, 64, 33, 40, 24, 59, 51, 33,  8, 58, 63, 60,  4, 46,\n",
       "       20,  6, 28, 58, 57, 57, 58, 62, 41, 26, 41, 13, 38, 46, 24])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xWcFwPwLSo05",
    "outputId": "050229f5-c72d-4f15-eae3-a41cdd86954f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input:\n",
      " b', this it is that makes your lady mourn!\\n\\nSecond Servant:\\nO, this is it that makes your servants dro'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"TqLbV?e3AE.Knn:Xel'SbqBRP$CgsUAWNg[UNK]DMT?I.J3VCQSd;SPAVDZ.lxsmpmG3,:xr$,,zyTaKtlT-sxu$gG'OsrrswbMb?YgK\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZOeWdgxNFDXq"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4HrXTACTdzY-",
    "outputId": "aa44b126-d9bf-4d1b-f213-349088d3218b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.189891, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\n",
    "    \"Prediction shape: \",\n",
    "    example_batch_predictions.shape,\n",
    "    \" # (batch_size, sequence_length, vocab_size)\",\n",
    ")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MAJfS5YoFiHf",
    "outputId": "c842cb3c-50c4-45e8-fcaf-136013781430",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "66.01559"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix, save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "UK-hmKjYVoll",
    "outputId": "8e06b20f-cef0-4ae3-e83d-d0d020f9991b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 21s 68ms/step - loss: 2.6975\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 11s 54ms/step - loss: 1.9742\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 11s 55ms/step - loss: 1.6955\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 12s 56ms/step - loss: 1.5364\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 11s 56ms/step - loss: 1.4392\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 11s 56ms/step - loss: 1.3727\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 11s 56ms/step - loss: 1.3204\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 1.2760\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 12s 58ms/step - loss: 1.2350\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 1.1944\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 12s 58ms/step - loss: 1.1543\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 12s 58ms/step - loss: 1.1122\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 12s 58ms/step - loss: 1.0685\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 12s 57ms/step - loss: 1.0216\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 12s 58ms/step - loss: 0.9730\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.9218\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.8689\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.8177\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.7681\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 12s 62ms/step - loss: 0.7215\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "iSBU1tHmlUSs"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float(\"inf\")] * len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
    "        )\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(\n",
    "            inputs=input_ids, states=states, return_state=True\n",
    "        )\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "fqMOuDutnOxK"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ST7PSyk9t1mT",
    "outputId": "cbef372a-32a4-4a35-84ee-e27b73a2f37e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROMEO:\n",
      "D should stay like to delivat;\n",
      "I put thee. Held Jurizel in tune?\n",
      "\n",
      "LADY GREY:\n",
      "The shadow will attend me for a husband.\n",
      "Conspiratry, what news few or ege?\n",
      "\n",
      "CLARENCE:\n",
      "O Doricles, they say amen? he was not going to safe\n",
      "And once we scouts: I doubt not out of driar;\n",
      "The other made me epiled from that truth abed,\n",
      "Beguiled in this aim, which to take up all, against the time:\n",
      "Under your lordship thou do art\n",
      "Which ne'er come in:\n",
      "And hark you that which shall give him, and\n",
      "in holy nughing when they live: pray you, commandment:'\n",
      "Practise smoots, protector, let me be in lawling time;\n",
      "But I will spend us love our cause to see, and show\n",
      "no strengthening of respect, whence should excelds,\n",
      "Cursed be sworn quickle, with his knave.'\n",
      "\n",
      "BENVOLIO:\n",
      "Go, by my provost, hearimorty most sorrow,\n",
      "And fall beloved, and, for the royal dight\n",
      "In all resign'd his head. Now I cermas,\n",
      "Their bratchers come to swearity, or it perceived\n",
      "I have done me from Menenius;\n",
      "Because I will ever service.\n",
      "Yet am I trust, Juliet would \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.790497064590454\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZkLu7Y8UCMT7",
    "outputId": "bd2eca9f-454c-4d93-a7f5-cf7b29969575",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nGentle nothing.\\n\\nMENENIUS:\\nWhat doth ye well?\\nWould yet you do be mature is in you\\nTo school-master with the Clarence how thyself and women\\nWhat I will prove his, perchance of York have to save\\nThy widdy hours in hoops and proclaims,\\nErpining the old pant that were but sake, and run,\\nNor the found shall scarlet him, brathers and Jove,' you\\nDribbet! O gravel in this viceous battle's death;\\nAnd you, good grace, would be gonerated,\\nAccording to the one.\\n\\nDUCHESS OF YORK:\\nO blessed sail, and loves not such about their honsur.\\nCome, talk not of perjury? But\\nOur Richard holding hard: having grown some devour\\nI prove a bold opportal, let my sweet soul\\nIs goodment I love myself.\\n\\nLEONTES:\\nFirst, ask your brother's pardon.\\n\\nISABELLA:\\nAy, but thou urgurst your majesty you have.\\n\\nPETRUCHIO:\\nBe king and drown; now what is your began?\\n\\nKING RICHARD III:\\nEven he, my lord, before his peace is fear;\\nAnd, then, they saw he were worn irpore mine art,\\nI have loved pewis of the point. The cases off,\\nWhic\"\n",
      " b\"ROMEO:\\nCome, Well, Juliet, do you but a scratch?\\nGood morrow, first begin, we'll do it through away.\\nO Rutland and all Claid of Sersonal: what come?\\n\\nBIONDELLO:\\nWhy, but hast thou, Warwick, in earth I now,\\nIs faither maids with outward.\\n\\nCAMILLO:\\nMy prayers told me I have set a king?\\nEdward thy grace! Where is he were queschio,\\nIs Edward my Bohamian, talk no less. But come your love,\\nAnd with thy beautity, of good time-hereaffer,\\nWere braught in gliest-lipsion.\\nBRidan, come, sir; for ever must ensue her on\\nher pestimation strikes, desires it speed.\\nLong and both o'er-rancies rude. Ary though\\nThey seek to bandy is dead! we'll buy myself answer;\\nAnd yet she is ifly nor enemy; nay, speak\\nMy men yet waged by himself answering.\\nAnd yet we still make hands cham in pieat,\\nHe should before us to their softence these\\nBy how or id go, Bolingbroke.\\n\\nKING EDWARD IV:\\nAy, but as though England's throat;\\nI will not speak it to you. You saw so; a little joy\\nHave I ne'er seven idle hate a child-swoon.\\n\\nBRUTU\"\n",
      " b\"ROMEO:\\nThere is no soleman, whom thou fishified, I will eat\\nUnder the device, who should puke on?\\n\\nAUFIDIUS:\\nWhy, here's my wagins must obscup will make\\nEnjoy is recaps of proceesing tongue.\\n\\nANGELO:\\nHappily with it with me;\\nAnso ambart, to see me speak to many house.\\nGood quarrel, York is too har than will I do,\\nHow say'st thou, that's my bones, and thou and to tell me\\nHow she was well: I would to God, my lord\\nBut stay the sun.\\nArise, and tell them, aboting of his pirgming brother\\nAside hereafter than mean to begin.\\n\\nLord Marshal:\\nSound, like a mighty power incured ta'ens.\\nSow York his penitence?\\n\\nBENVOLIO:\\nTo stript thee; in the common people that\\nHe would have suppler senators, place am\\nEspiring golden generation.\\n\\nANGELO:\\nSo please; insweath, what you have answer'd.\\nLest I accompanted he was not men\\nafterwo me: thou sounds the ture drunk cause,\\nTo which the sun preserve of them all; and thy valued\\nImpedies, their temptediest fears; whose laws\\na bird's foalish our own.\\n\\nKING HENRY VI:\\nEve\"\n",
      " b\"ROMEO:\\nFor we arrevel and shall say well; but I have said\\nThe true lips will be rouge of grievous king:\\nFor so I renowned and subden.\\nBut I would all the man, this land were of\\nMore an unstain for this fair lady's look.\\n\\nMARCIUS:\\nHow often being most thus husband,\\nI never in mine own law here I must,\\nThat must I their hearts to the rewal throught out:\\nThe cause why not a mouterate men, the obeshed age,\\nWhich now the bright son Edward, and state of brown\\nhis daughter's bones may know our kiss that heavens\\nfrom shedherd: but what of which I should do,\\nWere it before I seek a limit. Lady!\\nA heavy servant, or else hole, present,\\nWhose shade such leaves upon her pluck or observation.\\n\\nWARWICK:\\nShe hath, air, ambles great dare and sweet Mart's.\\n\\nHERMIONE:\\nThat's speak! I hole it in every babe;\\nNo, by your life, you must not home.\\n\\nHORTENSIO:\\nI should keep me wrong'd, I hear not 'Mange.\\n\\nKING HENRY VI:\\nWhy, I will make a trusty gentleman did us.\\n\\nPARIS:\\nWhy dost thou, were I thought so,\\nPlaster min\"\n",
      " b\"ROMEO:\\nThe hope there, give me thy head; and hath content me\\nTo be extimpting in the shore, thoughts, he\\nis truered; for you, my mercy dried too.\\n\\nHENRY BOLINGBROKE:\\nSweet Clarence, this being wen'd, cries 'twixt reign thy life,\\nHath your own sufferined steel against our hope;\\nAnd I for thee to be revenged on hel.\\n\\nGroop:\\nAy, but I'll see how duly's action clouds with\\nA purse; thousands Is,--there's the relation of the geor\\nTo fortune's gone. I am here,\\nWas not a slumber is about o' the bridal bed:\\nA forted in thy years' ne'er shall never drown\\nI am no beast, but only Warwick's joins?\\nNever must see this dreadful marques slumber in my smoint,\\nAllewith humy answer your for near.' 'Gissins him.\\nGood Grumio! Go to! Joy! Playous be thy just: being heard, I\\ngrant their officers: else thou hast advised:\\nA noble prince, as the precious by consent;\\nYields up from Kate, and we will make her chaste.\\n\\nFLORIZEL:\\nShe gates out of it\\nis to be tempted: when he was done to doom the garments\\nWould love my ho\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, \"\\n\\n\" + \"_\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlUQzwu6EXam"
   },
   "source": [
    "## Export the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Grk32H_CzsC",
    "outputId": "1ad741d5-5aeb-429c-91f3-323a6f014726",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7e2e34667040>, because it is not built.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, \"one_step\")\n",
    "one_step_reloaded = tf.saved_model.load(\"one_step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Z9bb_wX6Uuu"
   },
   "outputs": [],
   "source": [
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "text_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}